{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability.python.distributions as tfp\n",
    "from collections import deque\n",
    "import random as rand\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import scipy.stats as spst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self, obssize, actsize, sess, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: size of the states\n",
    "        actsize: size of the actions\n",
    "        \"\"\"\n",
    "        # BUILD PREDICTION GRAPH\n",
    "        # build the input\n",
    "        state = tf.placeholder(tf.float32, [None, obssize])\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        W1 = tf.Variable(initializer([obssize, 50]))\n",
    "        b1 = tf.Variable(initializer([50]))\n",
    "        W2 = tf.Variable(initializer([50, actsize]))\n",
    "        b2 = tf.Variable(initializer([actsize]))\n",
    "        \n",
    "        h1 = tf.nn.sigmoid(tf.matmul(state, W1) + b1)\n",
    "        h = tf.matmul(h1, W2) + b2\n",
    "        \n",
    "        prob = tf.nn.softmax(h, axis=1)  # prob is of shape [None, actsize], axis should be the one for each sample = axis = 1\n",
    "        probs_buffer = tf.placeholder(tf.float32, [5,None,actsize])\n",
    "        weights_ = tf.placeholder(tf.float32, [5])\n",
    "        weights = tf.nn.softmax(weights_)\n",
    "        \n",
    "        #########################   KL   ##########################\n",
    "\n",
    "        X = tfp.Categorical(probs=prob)\n",
    "        Y = tfp.Categorical(probs=probs_buffer)\n",
    "        distance = tfp.kl_divergence(X, Y)# a tensor containing the distances of distribution for each state and \n",
    "                                          # each policy from the replay buffer dim(5,len(OBS))\n",
    "        exploration_loss_KL_ = tf.reduce_mean(distance, axis=1)\n",
    "        exploration_loss_KL = tf.tensordot(exploration_loss_KL_, weights, axes=[0,0])\n",
    "        \n",
    "        #########################   L1   ###########################\n",
    "        Dif_ = probs_buffer - prob\n",
    "        norm_L1 = tf.norm(Dif_, axis=2,  ord=1)\n",
    "        exploration_loss_L1_ = tf.reduce_mean(distance, axis=1)\n",
    "        exploration_loss_L1 = tf.tensordot(exploration_loss_L1_, weights, axes=[0,0])\n",
    "\n",
    "        #########################   L2   ###########################\n",
    "        norm_L2 = tf.norm(Dif_, axis=2,  ord=2)\n",
    "        exploration_loss_L2_ = tf.reduce_mean(distance, axis=1)\n",
    "        exploration_loss_L2 = tf.tensordot(exploration_loss_L2_, weights, axes=[0,0])\n",
    "        \n",
    "        ########################   Linf   ############################\n",
    "        \n",
    "        norm_Linf = tf.norm(Dif_, axis=2,  ord=np.inf)\n",
    "        exploration_loss_Linf_ = tf.reduce_mean(distance, axis=1)\n",
    "        exploration_loss_Linf = tf.tensordot(exploration_loss_Linf_, weights, axes=[0,0])\n",
    "        ############################################################\n",
    "        \n",
    "        Q_estimate = tf.placeholder(tf.float32, [None])\n",
    "        actions = tf.placeholder(tf.int32, [None])\n",
    "        actions_one_hot = tf.one_hot(actions, actsize) # actions_one_hot will be a matrix of shape(n,2)\n",
    "        action_probabilities = tf.reduce_sum(prob * actions_one_hot, axis=1) # this will connect the action that was \n",
    "                                                                             # actually played to to its probability\n",
    "        \n",
    "        \n",
    "        surrogate_loss = -tf.reduce_mean(tf.log(action_probabilities)*Q_estimate)\n",
    "        explore_alpha = tf.placeholder(tf.float32,[])\n",
    "        loss_and_exploration_loss_KL = surrogate_loss - explore_alpha*exploration_loss_KL\n",
    "        loss_and_exploration_loss_L1 = surrogate_loss - explore_alpha*exploration_loss_L1\n",
    "        loss_and_exploration_loss_L2 = surrogate_loss - explore_alpha*exploration_loss_L2\n",
    "        loss_and_exploration_loss_Linf = surrogate_loss - explore_alpha*exploration_loss_Linf\n",
    "        self.train_op = optimizer.minimize(surrogate_loss)\n",
    "        self.train_explore_KL = optimizer.minimize(loss_and_exploration_loss_KL)\n",
    "        self.train_explore_L1 = optimizer.minimize(loss_and_exploration_loss_L1)\n",
    "        self.train_explore_L2 = optimizer.minimize(loss_and_exploration_loss_L2)\n",
    "        self.train_explore_Linf = optimizer.minimize(loss_and_exploration_loss_Linf)\n",
    "        \n",
    "        # some bookkeeping\n",
    "        self.state = state\n",
    "        self.prob = prob\n",
    "        self.actions = actions\n",
    "        self.Q_estimate = Q_estimate\n",
    "        self.loss = surrogate_loss\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "        self.explore_alpha = explore_alpha\n",
    "        self.exploration_loss_KL = exploration_loss_KL\n",
    "        self.exploration_loss_L1 = exploration_loss_L1\n",
    "        self.exploration_loss_L2 = exploration_loss_L2\n",
    "        self.exploration_loss_Linf = exploration_loss_Linf\n",
    "        self.probs_buffer = probs_buffer\n",
    "        self.weights = weights\n",
    "    \n",
    "    def compute_prob(self, states):\n",
    "        \"\"\"\n",
    "        compute prob over actions given states pi(a|s)\n",
    "        states: numpy array of size [numsamples, obssize]\n",
    "        return: numpy array of size [numsamples, actsize]\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.prob, feed_dict={self.state:states})\n",
    "\n",
    "    def train(self, states, actions, Qs):\n",
    "        \"\"\"\n",
    "        states: numpy array (states)\n",
    "        actions: numpy array (actions)\n",
    "        Qs: numpy array (Q values)\n",
    "        \"\"\"\n",
    "        self.sess.run(self.train_op, feed_dict={self.state:states, self.actions:actions, self.Q_estimate:Qs})\n",
    "        \n",
    "    def train_expl(self, states, actions, Qs, probs, expl_alpha, weights, distance_metric):\n",
    "        \"\"\"\n",
    "        states: numpy array (states)\n",
    "        actions: numpy array (actions)\n",
    "        Qs: numpy array (Q values)\n",
    "        batch_Q: numpy array as input to compute loss_explore\n",
    "        exploration_alpha: numpy array as input to loss_and_exploration_loss\n",
    "        distance_metrix: string representing method to use for exploration\n",
    "        \n",
    "        \"\"\"\n",
    "        if distance_metric == \"KL\":\n",
    "            return self.sess.run(self.train_explore_KL, feed_dict={self.state:states, self.actions:actions,\\\n",
    "                                                            self.Q_estimate:Qs, self.probs_buffer:probs,\\\n",
    "                                                            self.explore_alpha:expl_alpha, self.weights:weights})\n",
    "        elif distance_metric == \"L1\":\n",
    "            return self.sess.run(self.train_explore_L1, feed_dict={self.state:states, self.actions:actions,\\\n",
    "                                                            self.Q_estimate:Qs, self.probs_buffer:probs,\\\n",
    "                                                            self.explore_alpha:expl_alpha, self.weights:weights})\n",
    "        elif distance_metric == \"L2\":\n",
    "            return self.sess.run(self.train_explore_L2, feed_dict={self.state:states, self.actions:actions,\\\n",
    "                                                            self.Q_estimate:Qs, self.probs_buffer:probs,\\\n",
    "                                                            self.explore_alpha:expl_alpha, self.weights:weights})\n",
    "        \n",
    "        elif distance_metric == \"Linf\":\n",
    "            return self.sess.run(self.train_explore_Linf, feed_dict={self.state:states, self.actions:actions,\\\n",
    "                                                            self.Q_estimate:Qs, self.probs_buffer:probs,\\\n",
    "                                                            self.explore_alpha:expl_alpha, self.weights:weights})\n",
    "    \n",
    "    \n",
    "    def compute_exploration_loss(self, states, actions, Qs, probs, weights, distance_metric):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        batch_Q: numpy array as input to compute loss_explore\n",
    "        distance_metric: string \n",
    "        \"\"\"\n",
    "        if distance_metric == \"KL\":\n",
    "            return self.sess.run(self.exploration_loss_KL, feed_dict={self.state:states, self.actions:actions,\\\n",
    "                                                            self.Q_estimate:Qs, self.probs_buffer:probs,\\\n",
    "                                                            self.weights:weights})\n",
    "        elif distance_metric == \"L1\":\n",
    "            return self.sess.run(self.exploration_loss_L1, feed_dict={self.state:states, self.actions:actions,\\\n",
    "                                                            self.Q_estimate:Qs, self.probs_buffer:probs,\\\n",
    "                                                            self.weights:weights})\n",
    "        elif distance_metric == \"L2\":\n",
    "            return self.sess.run(self.exploration_loss_L2, feed_dict={self.state:states, self.actions:actions,\\\n",
    "                                                            self.Q_estimate:Qs, self.probs_buffer:probs,\\\n",
    "                                                            self.weights:weights})\n",
    "        elif distance_metric == \"Linf\":\n",
    "            return self.sess.run(self.exploration_loss_Linf, feed_dict={self.state:states, self.actions:actions,\\\n",
    "                                                            self.Q_estimate:Qs, self.probs_buffer:probs,\\\n",
    "                                                            self.weights:weights})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define value function as a class\n",
    "class ValueFunction(object):\n",
    "    def __init__(self, obssize, sess, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: size of states\n",
    "        \"\"\"\n",
    "        state = tf.placeholder(tf.float32, [None, obssize])\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        W1 = tf.Variable(initializer([obssize, 50]))\n",
    "        b1 = tf.Variable(initializer([50]))\n",
    "        W2 = tf.Variable(initializer([50, 1]))\n",
    "        b2 = tf.Variable(initializer([1]))\n",
    "        \n",
    "        h1 = tf.nn.sigmoid(tf.matmul(state, W1) + b1) # dim = [num_of_samples, 30]\n",
    "        predictions = tf.matmul(h1, W2) + b2 # dim = [num_of_samples, 1]\n",
    "        \n",
    "        self.predictions = predictions\n",
    "        self.obssize = obssize\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "        self.state = state\n",
    "        \n",
    "        \n",
    "        targets = tf.placeholder(tf.float32, [None])# first None because the number of sampled trajectories\n",
    "        # is not known beforehand and furthermore, the length of each trajectory might vary and is not constant. \n",
    "        # But in the end it is just a list of unkown size where each element is a number representing the value\n",
    "        # for the corresponding state in self.states\n",
    "        error = predictions - targets\n",
    "        loss = tf.reduce_mean(tf.square(error))\n",
    "        \n",
    "        self.targets = targets\n",
    "        self.train_op = optimizer.minimize(loss)\n",
    "\n",
    "    def compute_values(self, states):\n",
    "        \"\"\"\n",
    "        compute value function for given states\n",
    "        states: numpy array of size [numsamples, obssize]\n",
    "        return: numpy array of size [numsamples]\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.predictions, feed_dict={self.state:states})\n",
    "\n",
    "    def train(self, states, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array\n",
    "        targets: numpy array\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.train_op, feed_dict={self.state:states, self.targets:targets}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(r, gamma):\n",
    "    \"\"\" \n",
    "    take 1D float array of rewards and compute discounted reward \n",
    "    returns a list where the first element is the complete discounted reward for the whole trajectory (already summed),\n",
    "    the second element is the complete discounted reward for the trajectory starting at t=1 and so on...\n",
    "    \"\"\"\n",
    "    \n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for i in reversed(range(0,len(r))):\n",
    "        discounted_r[i] = running_sum * gamma + r[i]\n",
    "        running_sum = discounted_r[i]\n",
    "    return list(discounted_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target_update(from_scope, to_scope):\n",
    "    \"\"\"\n",
    "    from_scope: string representing the scope of the network FROM which the variables will be copied\n",
    "    to_scope: string representing the scope of the network TO which the variables will be copied\n",
    "    \"\"\"\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_scope)\n",
    "    op = []\n",
    "    for v1, v2 in zip(from_vars, to_vars):\n",
    "        op.append(v2.assign(v1))\n",
    "    return op    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement replay buffer\n",
    "import random\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batchsize)\n",
    "            \n",
    "        return batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune/add hyper-parameters numtrajs too high ? code is very slow\n",
    "# parameter initializations\n",
    "alpha = 1e-3  # learning rate for PG\n",
    "beta = 1e-3  # learning rate for baseline\n",
    "numtrajs = 15  # num of trajecories to collect at each iteration \n",
    "envname = \"CartPole-v1\"  # environment name\n",
    "gamma = .99  # discount\n",
    "episodes = 400 # total num of iterations\n",
    "maxlength = 5\n",
    "start = False\n",
    "exploration_alpha = 0.1\n",
    "distance_metric = \"Linf\" # Options are: Linf, L1, L2, KL\n",
    "distribution_buffer = \"Unif\" # Options are: Unif, exp_high_recent, exp_high_old\n",
    "# initialize environment\n",
    "env = gym.make(envname)\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n\n",
    "\n",
    "################# DO NOT CHANGE THIS PART###################\n",
    "#wrapper for accounting rewards\n",
    "rEpisode=0\n",
    "rList=[]\n",
    "\n",
    "def reset_decorate(func):\n",
    "    def func_wrapper():\n",
    "        global rList\n",
    "        global rEpisode\n",
    "        rList.append(rEpisode)\n",
    "        rEpisode=0\n",
    "        return(func())\n",
    "    return func_wrapper\n",
    "\n",
    "env.reset = reset_decorate(env.reset)\n",
    "\n",
    "def step_decorate(func):\n",
    "    def func_wrapper(action):\n",
    "        global rEpisode\n",
    "        s1, r, d, other = func(action)\n",
    "        rEpisode+=r\n",
    "        return(s1, r, d, other)\n",
    "    return func_wrapper\n",
    "\n",
    "env.step = step_decorate(env.step)\n",
    "\n",
    "def init():\n",
    "    rEpisode=0\n",
    "    rList=[]\n",
    "    return;\n",
    "#########################################################\n",
    "\n",
    "# sess\n",
    "sess = tf.Session()\n",
    "\n",
    "# optimizer\n",
    "optimizer_p = tf.train.AdamOptimizer(alpha)\n",
    "optimizer_v = tf.train.AdamOptimizer(beta)\n",
    "\n",
    "# initialize networks\n",
    "with tf.variable_scope(\"actor\"):\n",
    "    actor = Policy(obssize, actsize, sess, optimizer_p)  # policy initialization\n",
    "    \n",
    "baseline = ValueFunction(obssize, sess, optimizer_v)  # baseline initialization\n",
    "\n",
    "with tf.variable_scope(\"buffer_1\"):\n",
    "    buffer_1 = Policy(obssize, actsize, sess, optimizer_p)  # policy initialization\n",
    "with tf.variable_scope(\"buffer_2\"):\n",
    "    buffer_2 = Policy(obssize, actsize, sess, optimizer_p)  # policy initialization\n",
    "with tf.variable_scope(\"buffer_3\"):\n",
    "    buffer_3 = Policy(obssize, actsize, sess, optimizer_p)  # policy initialization\n",
    "with tf.variable_scope(\"buffer_4\"):\n",
    "    buffer_4 = Policy(obssize, actsize, sess, optimizer_p)  # policy initialization\n",
    "with tf.variable_scope(\"buffer_5\"):\n",
    "    buffer_5 = Policy(obssize, actsize, sess, optimizer_p)  # policy initialization\n",
    "    \n",
    "update1 = build_target_update(\"actor\", \"buffer_1\")\n",
    "update2 = build_target_update(\"actor\", \"buffer_2\")\n",
    "update3 = build_target_update(\"actor\", \"buffer_3\")\n",
    "update4 = build_target_update(\"actor\", \"buffer_4\")\n",
    "update5 = build_target_update(\"actor\", \"buffer_5\")\n",
    "\n",
    "# initialize tensorflow graphs\n",
    "sess.run(tf.global_variables_initializer())\n",
    "buffer = ReplayBuffer(maxlength)\n",
    "buffer_count = 1\n",
    "\n",
    "#logging info\n",
    "exploration_loss_log = np.array([])\n",
    "exploration_alpha_log = np.array([])\n",
    "# main iteration\n",
    "for ite in range(episodes):    \n",
    "\n",
    "    # trajs records for batch update\n",
    "    OBS = []  # observations\n",
    "    ACTS = []  # actions\n",
    "    ADS = []  # advantages (to update policy)\n",
    "    VAL = []  # value functions (to update baseline)\n",
    "\n",
    "    for num in range(numtrajs):\n",
    "        # record for each episode\n",
    "        obss = []  # observations\n",
    "        acts = []   # actions\n",
    "        rews = []  # instant rewards\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            prob = actor.compute_prob(np.expand_dims(obs,0))\n",
    "            action = np.random.choice(actsize, p=prob.flatten(), size=1)\n",
    "            newobs, reward, done, _ = env.step(action[0])\n",
    "\n",
    "            # record\n",
    "            obss.append(obs)\n",
    "            acts.append(action[0])\n",
    "            rews.append(reward)\n",
    "            #print(reward)\n",
    "\n",
    "            # update\n",
    "            obs = newobs\n",
    "\n",
    "        # compute returns from instant rewards for one whole trajectory\n",
    "        returns = discounted_rewards(rews, gamma)\n",
    "    \n",
    "        # record for batch update\n",
    "        VAL += returns # NOTE that the list of returns just gets extended. \n",
    "                       # There is no separate entry created for each trajectory\n",
    "        OBS += obss\n",
    "        ACTS += acts         \n",
    "    \n",
    "    # update baseline\n",
    "    VAL = np.array(VAL)# represents an array where the discounted reward lists are concatenated to each other.\n",
    "    # the size of VAL should be [numtrajs * len_of_traj_i, 1] where len_of_traj_i is variable depending on the length \n",
    "    # of each trajectory.\n",
    "    OBS = np.array(OBS)# represents an array where the list of states for all trajectories are concatenated.\n",
    "    # the size of OBS should be [numtrajs * len_of_traj_i, obssize]\n",
    "    ACTS = np.array(ACTS)\n",
    "    \n",
    "    baseline.train(OBS, VAL)  # update only one step\n",
    "    \n",
    "    # update policy\n",
    "    BAS = baseline.compute_values(OBS)  # compute baseline for variance reduction\n",
    "    ADS = VAL - np.squeeze(BAS,1) # computes advantages. An array of (targets)-(estimated from our network) for each\n",
    "                                  # state\n",
    "    \n",
    "    if distribution_buffer == \"Unif\":\n",
    "        weights = np.array([1,1,1,1,1])\n",
    "    elif distribution_buffer == \"exp_high_recent\": # recent experience has a bigger historic order value\n",
    "            weights = np.roll(np.array([1,2,3,4,5]), buffer_count)\n",
    "    elif distribution_buffer == \"exp_high_older\": # older experience has a bigger historic order value                      \n",
    "            weights = np.roll(np.array([5,4,3,2,1]), buffer_count)      \n",
    "                             \n",
    "            counter_ = r \n",
    "    if buffer_count==1 :\n",
    "        sess.run(update1)\n",
    "        buffer_count += 1\n",
    "    elif buffer_count==2 :\n",
    "        sess.run(update2)\n",
    "        buffer_count += 1\n",
    "    elif buffer_count==3 :\n",
    "        sess.run(update3)\n",
    "        buffer_count += 1\n",
    "    elif buffer_count==4 :\n",
    "        sess.run(update4)\n",
    "        buffer_count += 1\n",
    "    elif buffer_count==5 :\n",
    "        sess.run(update5)\n",
    "        start = True\n",
    "        buffer_count = 0\n",
    "    if start:    \n",
    "        prob1 = buffer_1.compute_prob(OBS)\n",
    "        prob2 = buffer_2.compute_prob(OBS)\n",
    "        prob3 = buffer_3.compute_prob(OBS)\n",
    "        prob4 = buffer_4.compute_prob(OBS)\n",
    "        prob5 = buffer_5.compute_prob(OBS)\n",
    "        probs_buffer = [prob1, prob2, prob3, prob4, prob5] # a tensor of dimension(5,len(OBS), 2)\n",
    "        \n",
    "        exploration_loss_log = np.append(exploration_loss_log, actor.compute_exploration_loss(\n",
    "                OBS, ACTS, ADS, probs_buffer, weights, distance_metric))\n",
    "        exploration_alpha_log = np.append(exploration_alpha_log, exploration_alpha)\n",
    "\n",
    "        actor.train_expl(OBS, ACTS, ADS, probs_buffer, exploration_alpha, weights, distance_metric) # update only one step\n",
    "        \n",
    "plt.plot(exploration_loss_log)\n",
    "plt.show()\n",
    "#plt.plot(exploration_alpha_log)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Evaluation: DO NOT CHANGE CODE HERE\n",
    "\n",
    "from numpy import convolve, ones\n",
    "def movingaverage(interval, window_size):\n",
    "    window= np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'valid')\n",
    "from pylab import plot\n",
    "%matplotlib inline \n",
    "\n",
    "rm=movingaverage(rList, 100)\n",
    "plot(rm)\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Evaluation: DO NOT CHANGE CODE HERE\n",
    "# after training, we will evaluate the performance of the agent\n",
    "# on a target environment\n",
    "eval_episodes = 100\n",
    "record = []\n",
    "env = gym.make('CartPole-v1')\n",
    "eval_mode = True\n",
    "for ite in range(eval_episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # epsilon greedy for exploration\n",
    "        if eval_mode:\n",
    "            p = actor.compute_prob(np.expand_dims(obs,0)).ravel()\n",
    "            action = np.random.choice(np.arange(2), size=1, p=p)[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        newobs, r, done, _ = env.step(action)\n",
    "        rsum += r\n",
    "        obs = newobs\n",
    "    \n",
    "    record.append(rsum)\n",
    "\n",
    "print(\"eval performance of PG agent: {}\".format(np.mean(record)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
