{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########################    ALWAYS RESTART KERNEL BEFORE RUNNING     ###########################\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability.python.distributions as tfp\n",
    "from collections import deque\n",
    "import random as rand\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(object):\n",
    "    def __init__(self, obssize, actsize, sess, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: size of the states\n",
    "        actsize: size of the actions\n",
    "        \"\"\"\n",
    "        # BUILD PREDICTION GRAPH\n",
    "        # build the input\n",
    "        state = tf.placeholder(tf.float32, [None, obssize])\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        W1 = tf.Variable(initializer([obssize, 50]))\n",
    "        b1 = tf.Variable(initializer([50]))\n",
    "        W2 = tf.Variable(initializer([50, actsize]))\n",
    "        b2 = tf.Variable(initializer([actsize]))\n",
    "        \n",
    "        h1 = tf.nn.sigmoid(tf.matmul(state, W1) + b1)\n",
    "        h = tf.matmul(h1, W2) + b2\n",
    "        \n",
    "        prob = tf.nn.softmax(h, axis=1)  # prob is of shape [None, actsize], axis should be the one for each sample = axis = 1\n",
    "        \n",
    "        # BUILD LOSS \n",
    "        Q_estimate = tf.placeholder(tf.float32, [None])\n",
    "        actions = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        actions_one_hot = tf.one_hot(actions, actsize) # actions_one_hot will be a matrix of shape(n,2)\n",
    "        action_probabilities = tf.reduce_sum(prob * actions_one_hot, axis=1) # this will connect the action that was \n",
    "                                                                             # actually played to to its probability\n",
    "        surrogate_loss = -tf.reduce_mean(tf.log(action_probabilities)*Q_estimate)\n",
    "        self.train_op = optimizer.minimize(surrogate_loss)\n",
    "        \n",
    "        # some bookkeeping\n",
    "        self.state = state\n",
    "        self.prob = prob\n",
    "        self.actions = actions\n",
    "        self.Q_estimate = Q_estimate\n",
    "        self.loss = surrogate_loss\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "    \n",
    "    def compute_prob(self, states):\n",
    "        \"\"\"\n",
    "        compute prob over actions given states pi(a|s)\n",
    "        states: numpy array of size [numsamples, obssize]\n",
    "        return: numpy array of size [numsamples, actsize]\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.prob, feed_dict={self.state:states})\n",
    "\n",
    "    def train(self, states, actions, Qs):\n",
    "        \"\"\"\n",
    "        states: numpy array (states)\n",
    "        actions: numpy array (actions)\n",
    "        Qs: numpy array (Q values)\n",
    "        \"\"\"\n",
    "        self.sess.run(self.train_op, feed_dict={self.state:states, self.actions:actions, self.Q_estimate:Qs})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define value function as a class\n",
    "class ValueFunction(object):\n",
    "    def __init__(self, obssize, sess, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: size of states\n",
    "        \"\"\"\n",
    "        state = tf.placeholder(tf.float32, [None, obssize])\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        W1 = tf.Variable(initializer([obssize, 50]))\n",
    "        b1 = tf.Variable(initializer([50]))\n",
    "        W2 = tf.Variable(initializer([50, 1]))\n",
    "        b2 = tf.Variable(initializer([1]))\n",
    "        \n",
    "        h1 = tf.nn.sigmoid(tf.matmul(state, W1) + b1) # dim = [num_of_samples, 30]\n",
    "        predictions = tf.matmul(h1, W2) + b2 # dim = [num_of_samples, 1]\n",
    "        \n",
    "        self.predictions = predictions\n",
    "        self.obssize = obssize\n",
    "        self.optimizer = optimizer\n",
    "        self.sess = sess\n",
    "        self.state = state\n",
    "        \n",
    "        \n",
    "        targets = tf.placeholder(tf.float32, [None])# first None because the number of sampled trajectories\n",
    "        # is not known beforehand and furthermore, the length of each trajectory might vary and is not constant. \n",
    "        # But in the end it is just a list of unkown size where each element is a number representing the value\n",
    "        # for the corresponding state in self.states\n",
    "        error = predictions - targets\n",
    "        loss = tf.reduce_mean(tf.square(error))\n",
    "        \n",
    "        self.targets = targets\n",
    "        self.train_op = optimizer.minimize(loss)\n",
    "\n",
    "    def compute_values(self, states):\n",
    "        \"\"\"\n",
    "        compute value function for given states\n",
    "        states: numpy array of size [numsamples, obssize]\n",
    "        return: numpy array of size [numsamples]\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.predictions, feed_dict={self.state:states})\n",
    "\n",
    "    def train(self, states, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array\n",
    "        targets: numpy array\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.train_op, feed_dict={self.state:states, self.targets:targets}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(r, gamma):\n",
    "    \"\"\" \n",
    "    take 1D float array of rewards and compute discounted reward \n",
    "    returns a list where the first element is the complete discounted reward for the whole trajectory (already summed),\n",
    "    the second element is the complete discounted reward for the trajectory starting at t=1 and so on...\n",
    "    \"\"\"\n",
    "    \n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_sum = 0\n",
    "    for i in reversed(range(0,len(r))):\n",
    "        discounted_r[i] = running_sum * gamma + r[i]\n",
    "        running_sum = discounted_r[i]\n",
    "    return list(discounted_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tobiasbraun/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/tobiasbraun/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# tune/add hyper-parameters numtrajs too high ? code is very slow\n",
    "# parameter initializations\n",
    "alpha = 1e-3  # learning rate for PG\n",
    "beta = 1e-3  # learning rate for baseline\n",
    "numtrajs = 15  # num of trajecories to collect at each iteration \n",
    "iterations = 100  # total num of iterations\n",
    "envname = \"CartPole-v1\"  # environment name\n",
    "gamma = .99  # discount\n",
    "episodes = 1\n",
    "\n",
    "# initialize environment\n",
    "env = gym.make(envname)\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n\n",
    "\n",
    "\n",
    "# sess\n",
    "sess = tf.Session()\n",
    "\n",
    "# optimizer\n",
    "optimizer_p = tf.train.AdamOptimizer(alpha)\n",
    "optimizer_v = tf.train.AdamOptimizer(beta)\n",
    "\n",
    "# initialize networks\n",
    "actor = Policy(obssize, actsize, sess, optimizer_p)  # policy initialization\n",
    "baseline = ValueFunction(obssize, sess, optimizer_v)  # baseline initialization\n",
    "\n",
    "# initialize tensorflow graphs\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#replay buffer\n",
    "# main iteration\n",
    "for ite in range(episodes):    \n",
    "\n",
    "    # trajs records for batch update\n",
    "    OBS = []  # observations\n",
    "    ACTS = []  # actions\n",
    "    ADS = []  # advantages (to update policy)\n",
    "    VAL = []  # value functions (to update baseline)\n",
    "\n",
    "    for num in range(numtrajs):\n",
    "        # record for each episode\n",
    "        obss = []  # observations\n",
    "        acts = []   # actions\n",
    "        rews = []  # instant rewards\n",
    "\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            prob = actor.compute_prob(np.expand_dims(obs,0))\n",
    "            action = np.random.choice(actsize, p=prob.flatten(), size=1)\n",
    "            newobs, reward, done, _ = env.step(action[0])\n",
    "\n",
    "            # record\n",
    "            obss.append(obs)\n",
    "            acts.append(action[0])\n",
    "            rews.append(reward)\n",
    "            #print(reward)\n",
    "\n",
    "            # update\n",
    "            obs = newobs\n",
    "\n",
    "        # compute returns from instant rewards for one whole trajectory\n",
    "        returns = discounted_rewards(rews, gamma)\n",
    "    \n",
    "        # record for batch update\n",
    "        VAL += returns # NOTE that the list of returns just gets extended. \n",
    "                       # There is no separate entry created for each trajectory\n",
    "        OBS += obss\n",
    "        ACTS += acts\n",
    "    \n",
    "    # update baseline\n",
    "    VAL = np.array(VAL)# represents an array where the discounted reward lists are concatenated to each other.\n",
    "    # the size of VAL should be [numtrajs * len_of_traj_i, 1] where len_of_traj_i is variable depending on the length \n",
    "    # of each trajectory.\n",
    "    OBS = np.array(OBS)# represents an array where the list of states for all trajectories are concatenated.\n",
    "    # the size of OBS should be [numtrajs * len_of_traj_i, obssize]\n",
    "    ACTS = np.array(ACTS)\n",
    "    \n",
    "    baseline.train(OBS, VAL)  # update only one step\n",
    "    \n",
    "    # update policy\n",
    "    BAS = baseline.compute_values(OBS)  # compute baseline for variance reduction\n",
    "    ADS = VAL - np.squeeze(BAS,1) # computes advantages. An array of (targets)-(estimated from our network) for each\n",
    "                                  # state\n",
    "\n",
    "    actor.train(OBS, ACTS, ADS)  # update only one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval performance of PG agent: 13.19\n"
     ]
    }
   ],
   "source": [
    "# Code Evaluation: DO NOT CHANGE CODE HERE\n",
    "# after training, we will evaluate the performance of the agent\n",
    "# on a target environment\n",
    "eval_episodes = 100\n",
    "record = []\n",
    "env = gym.make('CartPole-v1')\n",
    "eval_mode = True\n",
    "for ite in range(eval_episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # epsilon greedy for exploration\n",
    "        if eval_mode:\n",
    "            p = actor.compute_prob(np.expand_dims(obs,0)).ravel()\n",
    "            action = np.random.choice(np.arange(2), size=1, p=p)[0]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        newobs, r, done, _ = env.step(action)\n",
    "        rsum += r\n",
    "        obs = newobs\n",
    "    \n",
    "    record.append(rsum)\n",
    "\n",
    "print(\"eval performance of PG agent: {}\".format(np.mean(record)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
