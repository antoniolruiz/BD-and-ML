{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################    ALWAYS RESTART KERNEL BEFORE RUNNING     ###########################\n",
    "\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random as rand\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural net Q_\\theta(s,a) as a class\n",
    "\n",
    "class Qfunction(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, sess, optimizer):\n",
    "        \"\"\"\n",
    "        obssize: dimension of state space\n",
    "        actsize: dimension of action space\n",
    "        sess: sess to execute this Qfunction\n",
    "        optimizer: \n",
    "        \"\"\"\n",
    "\n",
    "        # build the prediction graph\n",
    "        state = tf.placeholder(tf.float32, [None, obssize])\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        W1 = tf.Variable(initializer([obssize,20]))\n",
    "        b1 = tf.Variable(initializer([20]))\n",
    "        W2 = tf.Variable(initializer([20, actsize]))\n",
    "        b2 = tf.Variable(initializer([actsize]))\n",
    "\n",
    "        h1 = tf.nn.sigmoid(tf.matmul(state, W1) + b1)\n",
    "        h = tf.matmul(h1, W2) + b2\n",
    "        \n",
    "        Qvalues = h #n*2 matrix\n",
    "        \n",
    "        targets = tf.placeholder(tf.float32, [None])\n",
    "        actions = tf.placeholder(tf.int32, [None])\n",
    "        action_ohe = tf.one_hot(actions, actsize) #  (n,2) matrix\n",
    "        \n",
    "        Qpreds = tf.reduce_sum(Qvalues * action_ohe, axis=1) #elementwise multiplication here\n",
    "        \n",
    "        td_errors = targets - Qpreds\n",
    "        \n",
    "        loss = tf.reduce_mean(tf.square(td_errors))\n",
    "\n",
    "        # optimization\n",
    "        self.train_op = optimizer.minimize(loss)\n",
    "    \n",
    "        # some bookkeeping\n",
    "        self.Qvalues = Qvalues\n",
    "        self.state = state\n",
    "        self.actions = actions\n",
    "        self.targets = targets\n",
    "        self.loss = loss\n",
    "        self.sess = sess\n",
    "    \n",
    "    def compute_Qvalues(self, states):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to the neural net, states should have\n",
    "        size [numsamples, obssize], where numsamples is the number of samples\n",
    "        output: Q values for these states. The output should have size \n",
    "        [numsamples, actsize] as numpy array\n",
    "        \"\"\"\n",
    "        return self.sess.run(self.Qvalues, feed_dict={self.state: states})\n",
    "\n",
    "    def train(self, states, actions, targets):\n",
    "        \"\"\"\n",
    "        states: numpy array as input to compute loss (s)\n",
    "        actions: numpy array as input to compute loss (a)\n",
    "        targets: numpy array as input to compute loss (Q targets)\n",
    "        \"\"\"\n",
    "        return self.sess.run([self.loss,self.train_op], feed_dict={self.state:states, self.actions:actions, self.targets:targets})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement replay buffer\n",
    "import random\n",
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, maxlength):\n",
    "        \"\"\"\n",
    "        maxlength: max number of tuples to store in the buffer\n",
    "        if there are more tuples than maxlength, pop out the oldest tuples\n",
    "        \"\"\"\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"\n",
    "        this function implements appending new experience tuple\n",
    "        experience: a tuple of the form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        \n",
    "    def pop(self):\n",
    "        \"\"\"\n",
    "        pop out the oldest tuples if self.number > self.maxlength\n",
    "        \"\"\"\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\"\n",
    "        this function samples 'batchsize' experience tuples\n",
    "        batchsize: size of the minibatch to be sampled\n",
    "        return: a list of tuples of form (s,a,r,s^\\prime)\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batchsize)\n",
    "            \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target_update(from_scope, to_scope):\n",
    "    \"\"\"\n",
    "    from_scope: string representing the scope of the network FROM which the variables will be copied\n",
    "    to_scope: string representing the scope of the network TO which the variables will be copied\n",
    "    \"\"\"\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=to_scope)\n",
    "    op = []\n",
    "    for v1, v2 in zip(from_vars, to_vars):\n",
    "        op.append(v2.assign(v1))\n",
    "    return op    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Learning with standard DQN.\n",
    "\"\"\"\n",
    "# parameter initializations\n",
    "lr = 1e-3  # learning rate for gradient update\n",
    "batchsize = 64  # batchsize for buffer sampling\n",
    "maxlength = 10000  # max number of tuples held by buffer\n",
    "envname = \"CartPole-v0\"  # environment name\n",
    "tau = 100  # time steps for target update\n",
    "episodes = 1000  # number of episodes to run\n",
    "initialsize = 500  # initial time steps before start updating\n",
    "epsilon = .05  # constant for exploration\n",
    "gamma = .99  # discount\n",
    "\n",
    "# initialize environment\n",
    "env = gym.make(envname)\n",
    "obssize = env.observation_space.low.size\n",
    "actsize = env.action_space.n\n",
    "\n",
    "# initialize tensorflow session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "# initialize networks\n",
    "with tf.variable_scope(\"principal\"):\n",
    "    Qprincipal = Qfunction(obssize, actsize, sess, optimizer)\n",
    "with tf.variable_scope(\"target\"):\n",
    "    Qtarget = Qfunction(obssize, actsize, sess, optimizer)\n",
    "\n",
    "# build ops\n",
    "update = build_target_update(\"principal\", \"target\")  # call sess.run(update) to copy\n",
    "                                                     # from principal to target\n",
    "\n",
    "# initialization of graph and buffer\n",
    "sess.run(tf.global_variables_initializer())\n",
    "buffer = ReplayBuffer(maxlength)\n",
    "sess.run(update)\n",
    "counter = 0\n",
    "# main iteration\n",
    "\n",
    "for i in range(episodes):\n",
    "    s = env.reset()\n",
    "    d = False\n",
    "    rsum = 0\n",
    "    while not d:\n",
    "       \n",
    "        if np.random.rand() < epsilon:\n",
    "            a = env.action_space.sample()\n",
    "         \n",
    "        else:\n",
    "            s_ = np.expand_dims(s, 0)\n",
    "            a = np.argmax(Qprincipal.compute_Qvalues(s_))\n",
    "        \n",
    "        s1, r, d, _ = env.step(a)\n",
    "        \n",
    "        rsum += r\n",
    "        counter += 1\n",
    "        buffer.append((s, a, r, s1, d))\n",
    "       \n",
    "\n",
    "        if (counter > initialsize):\n",
    "            sample = buffer.sample(batchsize)\n",
    "            sbatch = np.array([sam[0] for sam in sample])\n",
    "            abatch = np.array([sam[1] for sam in sample])\n",
    "            rbatch = np.array([sam[2] for sam in sample])\n",
    "            s1_batch = np.array([sam[3] for sam in sample])\n",
    "            d_batch = np.array([sam[4] for sam in sample])\n",
    "            targets = Qtarget.compute_Qvalues(s1_batch)\n",
    "            target_ = rbatch + gamma*np.max(targets, 1)*(1-d_batch)\n",
    "            Qprincipal.train(sbatch, abatch, target_)\n",
    "            \n",
    "            if (counter%tau == 0):\n",
    "                sess.run(update)\n",
    "                \n",
    "        s = s1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval performance of DQN agent: 125.89\n"
     ]
    }
   ],
   "source": [
    "# Code Evaluation: DO NOT CHANGE CODE HERE\n",
    "# after training, we will evaluate the performance of the agent\n",
    "# on a target environment\n",
    "eval_episodes = 100\n",
    "record = []\n",
    "env = gym.make('CartPole-v0')\n",
    "eval_mode = True\n",
    "for ite in range(eval_episodes):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    \n",
    "    while not done:\n",
    "        if eval_mode:\n",
    "            values = Qprincipal.compute_Qvalues(np.expand_dims(obs,0))\n",
    "            action = np.argmax(values.flatten())\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        newobs, r, done, _ = env.step(action)\n",
    "        rsum += r\n",
    "        obs = newobs\n",
    "    \n",
    "    record.append(rsum)\n",
    "\n",
    "print(\"eval performance of DQN agent: {}\".format(np.mean(record)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
